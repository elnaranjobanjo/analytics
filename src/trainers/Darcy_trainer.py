import dolfin
import fenics as fe
import json
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import os
import pandas as pd
from dataclasses import dataclass, field

import Darcy_generator as Dg


@dataclass
class DarcyTrainingParams:
    formulation: str = "primal"
    mesh: fe.Mesh = fe.UnitSquareMesh(5, 5)
    degree: int = 1
    f: str = "10"
    A_matrix_params: list[list[float], list[float]] = field(
        default_factory=lambda: [[5, 10], [5, 10]]
    )
    epochs: int = 10
    learn_rate: float = 0.001
    dataless: bool = True
    number_of_data_points: int = 100
    percentage_for_validation: float = 0.2
    batch_size: int = 10


class Darcy_nn(nn.Module):
    def __init__(
        self, input_size: int = 3, hidden_size: int = 64, output_size: int = 1
    ):
        super(Darcy_nn, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, hidden_size)
        self.fc4 = nn.Linear(hidden_size, hidden_size)
        self.fc5 = nn.Linear(hidden_size, hidden_size)
        self.fc6 = nn.Linear(hidden_size, output_size)
        self.activation = nn.GELU()

    def forward(self, x: torch.tensor):
        x = self.activation(self.fc1(x))
        x = self.activation(self.fc2(x))
        x = self.activation(self.fc3(x))
        x = self.activation(self.fc4(x))
        x = self.activation(self.fc5(x))
        x = self.fc6(x)
        return x


class PDE_Loss(nn.Module):
    def __init__(self):
        super(PDE_Loss, self).__init__()
        self.lossfun = nn.MSELoss()


# Given a triplet [eig_1,eig_2,theta] the factory class
# trains a pair of darcy nets u,p solving
#       -div u = f
#    A grad p = u
#           p = 0  b.c.
# with A sym. pos. def. with eigen vals eig_1 and eig_2
# The matrix of eigvecs is the rotation matrix generated by theta
class Darcy_dual_PDE_Loss(PDE_Loss):
    def __init__(self, params: DarcyTrainingParams):
        super().__init__()
        self.formulation = Dg.Darcy_dual_formulation(params)
        self.f = torch.from_numpy(self.formulation.get_rhs_vector())
        self.lossfun = nn.MSELoss()

    def forward(
        self, u_dofs: torch.tensor, p_dofs: torch.tensor, A_matrix_params: list
    ):
        return self.lossfun(
            torch.matmul(
                torch.from_numpy(
                    self.formulation.assemble_linear_system(A_matrix_params)
                ),
                torch.cat((u_dofs, p_dofs), dim=0),
            ),
            self.f,
        )


# Given a triplet [eig_1,eig_2,theta] the factory class
# Encodes the PDE loss func for:
#       -div A grad p = f
#           p = 0  b.c.
class Darcy_primal_PDE_Loss(PDE_Loss):
    def __init__(self, params: DarcyTrainingParams):
        super().__init__()
        self.formulation = Dg.Darcy_primal_formulation(params)
        self.f = torch.from_numpy(self.formulation.get_rhs_vector())

    def forward(self, p_dofs: torch.tensor, A_matrix_params: list):
        return self.lossfun(
            torch.matmul(
                torch.from_numpy(
                    self.formulation.assemble_linear_system(A_matrix_params)
                ),
                p_dofs,
            ),
            self.f,
        )

    # def forward(self, p_dofs: torch.tensor, A_matrix_params: list):
    #     return 0.5 * torch.dot(
    #         torch.matmul(
    #             torch.from_numpy(
    #                 self.formulation.assemble_linear_system(A_matrix_params)
    #             ),
    #             p_dofs,
    #         ),
    #         p_dofs,
    #     ) - torch.dot(self.f, p_dofs)


class nn_solver:
    def __init__(self):
        pass

    def save_mesh(self, directory_path: str) -> None:
        mesh_file = dolfin.File(os.path.join(directory_path, "mesh.xml"))
        mesh_file << self.model_space.mesh()

    def load_mesh_and_device(self, directory_path: str) -> (fe.Mesh, int, torch.device):
        with open(os.path.join(directory_path, "degree.json"), "r") as json_file:
            degree_json = json.load(json_file)

        if torch.cuda.is_available():
            device = torch.device("cuda")
        else:
            device = torch.device("cpu")
        return (
            fe.Mesh(os.path.join(directory_path, "mesh.xml")),
            degree_json["degree"],
            device,
        )


class Darcy_primal_nn_solver(nn_solver):
    def __init__(self):
        super().__init__()
        pass

    def init_from_nets(
        self,
        p_net: Darcy_nn,
        model_space: fe.FunctionSpace,
    ):
        self.p_net = p_net
        self.model_space = model_space
        return self

    def to_fenics(self, A_matrix_params: list) -> fe.Function:
        bias_term = torch.tensor([1], dtype=A_matrix_params.dtype)
        x_with_bias = torch.cat(
            (bias_term, torch.tensor(np.array(A_matrix_params))), dim=0
        )

        p_dofs = self.p.forward(x_with_bias).detach().numpy()
        p = fe.Function(self.model_space)
        p.vector().set_local(p_dofs)
        return p

    def save(self, directory_path: str):
        if not os.path.exists(directory_path):
            os.makedirs(directory_path)

        self.save_mesh(directory_path)
        dict = {"degree": self.model_space.ufl_element().degree()}
        with open(os.path.join(directory_path, "degree.json"), "w") as json_file:
            json.dump(dict, json_file)
        torch.save(self.p_net.state_dict(), os.path.join(directory_path, "p_net.pt"))

    def load(self, directory_path: str):
        mesh, degree, device = self.load_model_space_specs(directory_path)
        self.model_space = Dg.Darcy_primal_formulation(
            Dg.DarcySimParams(mesh=mesh, degree=degree)
        ).get_model_space()

        self.p_net = Darcy_nn(
            input_size=4,
            hidden_size=int((2 / 3) * (4 + self.model_space.dim())),
            output_size=self.model_space.dim(),
        )
        self.p_net.load_state_dict(torch.load(directory_path + "/p_net.pt"))
        self.p_net.to(device)
        return self


class Darcy_dual_nn_solver(nn_solver):
    def __init__(self):
        super().__init__()
        pass

    def init_from_nets(
        self,
        u_net: Darcy_nn,
        p_net: Darcy_nn,
        model_space: fe.FunctionSpace,
    ):
        self.u_net = u_net
        self.p_net = p_net
        self.model_space = model_space
        return self

    def to_fenics(self, A_matrix_params: list) -> (fe.Function, fe.Function):
        bias_term = torch.tensor([1], dtype=A_matrix_params.dtype)
        x_with_bias = torch.cat(
            (bias_term, torch.tensor(np.array(A_matrix_params))), dim=0
        )
        u_dofs = self.u.forward(x_with_bias).detach().numpy()
        p_dofs = self.p.forward(x_with_bias).detach().numpy()

        (u, p) = fe.Function(self.model_space).split()

        u.vector().set_local(u_dofs)
        p.vector().set_local(p_dofs)
        return (u, p)

    def save(self, directory_path: str):
        if not os.path.exists(directory_path):
            os.makedirs(directory_path)

        self.save_mesh(directory_path)
        dict = {"degree": self.model_space.sub(0).ufl_element().degree()}
        with open(os.path.join(directory_path, "degree.json"), "w") as json_file:
            json.dump(dict, json_file)

        torch.save(self.u_net.state_dict(), os.path.join(directory_path, "u_net.pt"))
        torch.save(self.p_net.state_dict(), os.path.join(directory_path, "p_net.pt"))

    def load(self, directory_path: str):
        mesh, degree, device = self.load_model_space_specs(directory_path)
        self.model_space = Dg.Darcy_dual_formulation(
            Dg.DarcySimParams(mesh=mesh, degree=degree)
        ).get_model_space()

        self.u_net = Darcy_nn(
            input_size=4,
            hidden_size=int((2 / 3) * (4 + self.model_space.sub(0).dim())),
            output_size=self.model_space.sub(0).dim(),
        )
        self.u_net.load_state_dict(torch.load(directory_path + "/u_net.pt"))
        self.u_net.to(device)
        self.p_net = Darcy_nn(
            input_size=4,
            hidden_size=int((2 / 3) * (4 + self.model_space.sub(1).dim())),
            output_size=self.model_space.sub(1).dim(),
        )
        self.p_net.load_state_dict(torch.load(directory_path + "/p_net.pt"))
        self.p_net.to(device)
        return self


class PDE_trainer:
    def __init__(self):
        if torch.cuda.is_available():
            device = torch.device("cuda")
        else:
            device = torch.device("cpu")
        input_size = 4
        return device, input_size


class Darcy_dual_trainer(PDE_trainer):
    def __init__(self, params: DarcyTrainingParams):
        device, input_size = super().__init__()
        self.PDE_loss = Darcy_dual_PDE_Loss(params)
        u_output_size = self.PDE_loss.formulation.get_model_space().sub(0).dim()
        u_hidden_size = int((2 / 3) * (input_size + u_output_size))
        p_output_size = self.PDE_loss.formulation.get_model_space().sub(1).dim()
        p_hidden_size = int((2 / 3) * (input_size + p_output_size))

        self.u_net = Darcy_nn(
            input_size=input_size,
            hidden_size=u_hidden_size,
            output_size=u_output_size,
        ).to(device)

        self.p_net = Darcy_nn(
            input_size=input_size,
            hidden_size=p_hidden_size,
            output_size=p_output_size,
        ).to(device)

        self.u_optimizer = torch.optim.Adam(self.u_net.parameters(), params.learn_rate)
        self.p_optimizer = torch.optim.Adam(self.p_net.parameters(), params.learn_rate)

        self.data_loss = nn.MSELoss()

    def calculate_PDE_loss(self, x_batch):
        loss = 0
        for x in x_batch:
            bias_term = torch.tensor([1], dtype=x.dtype)
            x_with_bias = torch.cat((bias_term, x), dim=0)
            loss += self.PDE_loss(self.u_net(x_with_bias), self.p_net(x_with_bias), x)

        return loss / len(x_batch)

    def calculate_data_loss(self, x_batch, y_batch):
        loss = 0
        for x, y in zip(x_batch, y_batch):
            bias_term = torch.tensor([1], dtype=x.dtype)
            x_with_bias = torch.cat((bias_term, x), dim=0)
            loss += self.Data_loss(
                torch.cat((self.u_net(x_with_bias), self.p_net(x_with_bias)), dim=0), y
            )

        return loss / len(x_batch)

    def get_nn_solver(self):
        return Darcy_dual_nn_solver().init_from_nets(
            self.u_net, self.p_net, self.PDE_loss.formulation.get_model_space()
        )

    def train(self):
        self.u_net.train()
        self.p_net.train()

    def step(self):
        self.u_optimizer.step()
        self.p_optimizer.step()


class Darcy_primal_trainer(PDE_trainer):
    def __init__(self, params: DarcyTrainingParams):
        device, input_size = super().__init__()
        self.PDE_loss = Darcy_primal_PDE_Loss(params)
        output_size = self.PDE_loss.formulation.get_model_space().dim()
        # hidden_size = int((2 / 3) * (input_size + output_size))
        hidden_size = 500

        self.p_net = Darcy_nn(
            input_size=input_size,
            hidden_size=hidden_size,
            output_size=output_size,
        ).to(device)

        self.p_optimizer = torch.optim.Adam(
            self.p_net.parameters(), lr=params.learn_rate
        )

        self.data_loss = nn.MSELoss()

    def calculate_PDE_loss(self, x_batch):
        loss = 0
        for x in x_batch:
            bias_term = torch.tensor([1], dtype=x.dtype)
            x_with_bias = torch.cat((bias_term, x), dim=0)
            loss += self.PDE_loss(self.p_net(x_with_bias), x)

        return loss / len(x_batch)

    # def calculate_data_loss(self, x_batch, y_batch):
    #     loss = 0
    #     for x, y in zip(x_batch, y_batch):
    #         bias_term = torch.tensor([1], dtype=x.dtype)
    #         x_with_bias = torch.cat((bias_term, x), dim=0)
    #         loss += self.Data_loss(self.p_net(x_with_bias), y)

    #     return loss / len(x_batch)
    def calculate_data_loss(self, X_batch, Y_batch):
        X_batch = torch.cat((X_batch, torch.ones(X_batch.shape[0], 1)), dim=1)
        return self.data_loss(self.p_net(X_batch), Y_batch)

    def get_nn_solver(self):
        return Darcy_primal_nn_solver().init_from_nets(
            self.p_net, self.PDE_loss.formulation.get_model_space()
        )

    def train(self):
        self.p_net.train()

    def step(self):
        self.p_optimizer.step()


class Darcy_nn_Factory:
    def __init__(self, params: DarcyTrainingParams):
        if params.formulation == "primal":
            self.trainer = Darcy_primal_trainer(params)
        elif params.formulation == "dual":
            self.trainer = Darcy_dual_trainer(params)

        self.epochs = params.epochs
        self.dataless = params.dataless

    def fit(
        self,
        training_data: list,
        validation_data: list,
        batch_size: int,
        output_dir: str,
        verbose: bool = False,
    ):
        if self.dataless:
            training_set = TensorDataset(torch.tensor(training_data[0]))
            validation_set = TensorDataset(torch.tensor(validation_data[0]))
        else:
            training_set = TensorDataset(
                torch.tensor(training_data[0]), torch.tensor(training_data[1])
            )
            validation_set = TensorDataset(
                torch.tensor(validation_data[0]), torch.tensor(validation_data[1])
            )
        training_loader = DataLoader(
            dataset=training_set, batch_size=batch_size, shuffle=False
        )
        validation_loader = DataLoader(
            dataset=validation_set, batch_size=batch_size, shuffle=False
        )

        losses = []
        for i in range(self.epochs):
            training_loss, validation_loss = self.one_grad_descent_iter(
                training_loader,
                validation_loader,
            )
            losses.append([training_loss, validation_loss])
            # with open(os.path.join(output_dir, "log.txt"), "a") as file:
            #     file.write(f"epoch number {i}")
            #     file.write(f"training loss = {training_loss}\n")
            #     file.write(f"validation loss = {validation_loss}\n")

            if verbose:
                print(f"epoch = {i+1}")
                print(f"training loss = {training_loss}")
                print(f"validation loss = {validation_loss}\n")
        if self.dataless:
            out_csv = pd.DataFrame(
                [[l[0][0], l[1][0]] for l in losses], columns=["training", "validation"]
            )
            out_csv.to_csv(os.path.join(output_dir, "loss.csv"), index=False)
        else:
            out_csv = pd.DataFrame(
                [
                    [l[0][0], l[0][1], l[0][2], l[1][0], l[1][1], l[1][2]]
                    for l in losses
                ],
                columns=[
                    "total_training",
                    "PDE_training_loss",
                    "Data_training_loss",
                    "total_validation",
                    "PDE_validation_loss",
                    "Data_validation_loss",
                ],
            )
            out_csv.to_csv(os.path.join(output_dir, "loss.csv"), index=False)

        return self.trainer.get_nn_solver()

    def one_grad_descent_iter(
        self,
        training_loader,
        validation_loader,
    ):
        training_loss = [[], [], []]
        validation_loss = [[], [], []]

        self.trainer.train()
        for batch in training_loader:
            if self.dataless:
                x_batch = batch[0]
                total_loss = 0
                training_loss[2].append(0)
            else:
                x_batch, y_batch = batch
                Data_loss = self.trainer.calculate_data_loss(x_batch, y_batch)
                total_loss = Data_loss
                training_loss[2].append(Data_loss.item())

            x_batch.requires_grad = True

            PDE_loss = self.trainer.calculate_PDE_loss(x_batch)
            # PDE_loss = 0
            total_loss = total_loss + PDE_loss
            # training_loss[1].append(PDE_loss.item())
            training_loss[1].append(0)
            training_loss[0].append(total_loss.item())

            total_loss.backward()
            self.trainer.step()

            validation_loss[0].append(0)
            validation_loss[1].append(0)
            validation_loss[2].append(0)

        # for batch in validation_loader:
        #     if self.dataless:
        #         x_batch = batch[0]
        #         total_val_loss = 0
        #         validation_loss[2].append(0)
        #     else:
        #         x_batch, y_batch = batch
        #         Data_loss = self.trainer.calculate_data_loss(x_batch, y_batch)
        #         total_val_loss = Data_loss
        #         validation_loss[2].append(Data_loss.item())

        #     # PDE_loss = self.trainer.calculate_PDE_loss(x_batch)
        #     PDE_loss = 0
        #     total_val_loss = total_val_loss + PDE_loss
        #     validation_loss[1].append(0)
        #     # validation_loss[1].append(PDE_loss.item())
        #     validation_loss[0].append(total_val_loss.item())

        return [np.array(loss).mean(axis=0) for loss in training_loss], [
            np.array(loss).mean(axis=0) for loss in validation_loss
        ]


# class Darcy_nn_Factory:
#     def __init__(self, params: DarcyTrainingParams):
#         self.input_size = 4
#         if params.formulation == "primal":
#             self.PDE_loss = Darcy_primal_PDE_Loss(params)
#             self.p_output_size = (
#                 self.PDE_loss.formulation.get_model_space().sub(1).dim()
#             )
#             self.hidden_size = int((2 / 3) * (self.input_size + self.u_output_size))
#         elif params.formulation == "dual":
#             self.PDE_loss = Darcy_dual_PDE_Loss(params)
#             self.u_output_size = (
#                 self.PDE_loss.formulation.get_model_space().sub(0).dim()
#             )
#             self.u_hidden_size = int((2 / 3) * (self.input_size + self.u_output_size))
#             self.p_output_size = (
#                 self.PDE_loss.formulation.get_model_space().sub(1).dim()
#             )
#             self.p_hidden_size = int((2 / 3) * (self.input_size + self.p_output_size))
#         else:
#             return ValueError(f"Unknown formulation type Darcy {params.formulation}")

#         self.epochs = params.epochs
#         self.learn_rate = params.learn_rate

#         self.dataless = params.dataless
#         if not self.dataless:
#             self.Data_loss = nn.MSELoss()

#     def fit(
#         self,
#         training_data: list,
#         validation_data: list,
#         batch_size: int,
#         output_dir: str,
#         verbose: bool = False,
#     ) -> Darcy_dual_nn_Solver:
#         if torch.cuda.is_available():
#             device = torch.device("cuda")
#         else:
#             device = torch.device("cpu")
#         u_net = Darcy_nn(
#             input_size=self.input_size,
#             hidden_size=self.u_hidden_size,
#             output_size=self.u_output_size,
#         ).to(device)

#         p_net = Darcy_nn(
#             input_size=self.input_size,
#             hidden_size=self.p_hidden_size,
#             output_size=self.p_output_size,
#         ).to(device)

#         u_optimizer = torch.optim.Adam(u_net.parameters(), lr=self.learn_rate)
#         p_optimizer = torch.optim.Adam(p_net.parameters(), lr=self.learn_rate)

#         if self.dataless:
#             training_set = TensorDataset(torch.tensor(training_data[0]))
#             validation_set = TensorDataset(torch.tensor(validation_data[0]))
#         else:
#             training_set = TensorDataset(
#                 torch.tensor(training_data[0]), torch.tensor(training_data[1])
#             )
#             validation_set = TensorDataset(
#                 torch.tensor(validation_data[0]), torch.tensor(validation_data[1])
#             )
#         training_loader = DataLoader(
#             dataset=training_set, batch_size=batch_size, shuffle=False
#         )
#         validation_loader = DataLoader(
#             dataset=validation_set, batch_size=batch_size, shuffle=False
#         )

#         losses = []
#         for i in range(self.epochs):
#             training_loss, validation_loss = self.one_grad_descent_iter(
#                 training_loader,
#                 validation_loader,
#                 u_net,
#                 p_net,
#                 u_optimizer,
#                 p_optimizer,
#             )
#             losses.append([training_loss, validation_loss])
#             # with open(os.path.join(output_dir, "log.txt"), "a") as file:
#             #     file.write(f"epoch number {i}")
#             #     file.write(f"training loss = {training_loss}\n")
#             #     file.write(f"validation loss = {validation_loss}\n")

#             if verbose:
#                 print(f"epoch = {i+1}")
#                 print(f"training loss = {training_loss}")
#                 print(f"validation loss = {validation_loss}\n")
#         if self.dataless:
#             out_csv = pd.DataFrame(
#                 [[l[0][0], l[1][0]] for l in losses], columns=["training", "validation"]
#             )
#             out_csv.to_csv(os.path.join(output_dir, "loss.csv"), index=False)
#         else:
#             out_csv = pd.DataFrame(
#                 [
#                     [l[0][0], l[0][1], l[0][2], l[1][0], l[1][1], l[1][2]]
#                     for l in losses
#                 ],
#                 columns=[
#                     "total_training",
#                     "PDE_training_loss",
#                     "Data_training_loss",
#                     "total_validation",
#                     "PDE_validation_loss",
#                     "Data_validation_loss",
#                 ],
#             )
#             out_csv.to_csv(os.path.join(output_dir, "loss.csv"), index=False)

#         return Darcy_dual_nn_Solver().init_from_nets(
#             u_net, p_net, self.PDE_loss.formulation.get_model_space()
#         )

#     def one_grad_descent_iter(
#         self, training_loader, validation_loader, u_net, p_net, u_optimizer, p_optimizer
#     ):
#         training_loss = [[], [], []]
#         validation_loss = [[], [], []]

#         u_net.train()
#         p_net.train()
#         for batch in training_loader:
#             if self.dataless:
#                 x_batch = batch[0]
#                 total_loss = 0
#                 training_loss[2].append(0)
#             else:
#                 x_batch, y_batch = batch
#                 Data_loss = self.calculate_Data_loss(u_net, p_net, x_batch, y_batch)
#                 total_loss = Data_loss
#                 training_loss[2].append(Data_loss.item())

#             PDE_loss = self.calculate_PDE_loss(u_net, p_net, x_batch)
#             total_loss = total_loss + PDE_loss
#             training_loss[1].append(PDE_loss.item())
#             training_loss[0].append(total_loss.item())

#             total_loss.backward()
#             u_optimizer.step()
#             p_optimizer.step()

#         for batch in validation_loader:
#             if self.dataless:
#                 x_batch = batch[0]
#                 total_val_loss = 0
#                 validation_loss[2].append(0)
#             else:
#                 x_batch, y_batch = batch
#                 Data_loss = self.calculate_Data_loss(u_net, p_net, x_batch, y_batch)
#                 total_val_loss = Data_loss
#                 validation_loss[2].append(Data_loss.item())

#             PDE_loss = self.calculate_PDE_loss(u_net, p_net, x_batch)
#             total_val_loss = total_val_loss + PDE_loss
#             validation_loss[1].append(PDE_loss.item())
#             validation_loss[0].append(total_val_loss.item())

#         return [np.array(loss).mean(axis=0) for loss in training_loss], [
#             np.array(loss).mean(axis=0) for loss in validation_loss
#         ]

#     def calculate_PDE_loss(self, u_net, p_net, x_batch):
#         loss = 0
#         for x in x_batch:
#             bias_term = torch.tensor([1], dtype=x.dtype)
#             x_with_bias = torch.cat((bias_term, x), dim=0)
#             loss += self.PDE_loss(u_net(x_with_bias), p_net(x_with_bias), x)

#         return loss / len(x_batch)

#     def calculate_Data_loss(self, u_net, p_net, x_batch, y_batch):
#         loss = 0
#         for x, y in zip(x_batch, y_batch):
#             bias_term = torch.tensor([1], dtype=x.dtype)
#             x_with_bias = torch.cat((bias_term, x), dim=0)
#             loss += self.Data_loss(
#                 torch.cat((u_net(x_with_bias), p_net(x_with_bias)), dim=0), y
#             )

#         return loss / len(x_batch)
